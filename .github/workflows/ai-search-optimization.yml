# Create a dynamic and comprehensive AI-friendly sitemap
echo "ðŸ¤– Creating dynamic AI sitemap..."
echo '<?xml version="1.0" encoding="UTF-8"?>' > public/ai-sitemap.xml
echo '<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">' >> public/ai-sitemap.xml

# Add main static pages with high priority
echo '  <url><loc>https://justlegalsolutions.org/</loc><lastmod>$(date -I)</lastmod><priority>1.0</priority></url>' >> public/ai-sitemap.xml
echo '  <url><loc>https://justlegalsolutions.org/services/</loc><lastmod>$(date -I)</lastmod><priority>0.9</priority></url>' >> public/ai-sitemap.xml
echo '  <url><loc>https://justlegalsolutions.org/process-server-tulsa/</loc><lastmod>$(date -I)</lastmod><priority>0.9</priority></url>' >> public/ai-sitemap.xml
echo '  <url><loc>https://justlegalsolutions.org/pricing/</loc><lastmod>$(date -I)</lastmod><priority>0.8</priority></url>' >> public/ai-sitemap.xml
echo '  <url><loc>https://justlegalsolutions.org/why-choose-us/</loc><lastmod>$(date -I)</lastmod><priority>0.8</priority></url>' >> public/ai-sitemap.xml

# Dynamically find and add all local SEO pages
find app/(main)/seo -name "page.tsx" | while read -r page; do
  # Extract the directory name (e.g., process-server-bixby)
  location_slug=$(basename $(dirname "$page"))
  
  # Construct the full URL
  url="https://justlegalsolutions.org/seo/$location_slug/"
  
  # Add to the sitemap
  echo "    <url><loc>$url</loc><lastmod>$(date -I)</lastmod><priority>0.9</priority></url>" >> public/ai-sitemap.xml
done

echo '</urlset>' >> public/ai-sitemap.xml
echo "âœ… AI Sitemap created with all local SEO pages."

# Update robots.txt for AI crawlers
echo "" >> public/robots.txt
echo "# AI Search Optimization - $(date)" >> public/robots.txt
echo "User-agent: ChatGPT-User" >> public/robots.txt
echo "Allow: /" >> public/robots.txt
echo "User-agent: Claude-Web" >> public/robots.txt
echo "Allow: /" >> public/robots.txt
echo "User-agent: Google-Extended" >> public/robots.txt
echo "Allow: /" >> public/robots.txt
